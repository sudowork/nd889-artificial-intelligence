{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "\n",
    "* Excellent example of a **computationally hard** problem.\n",
    "* **Computationally hard**: Problems with high time or space complexity.\n",
    "* Example: Manchester -> Sheffield.\n",
    "    * Search problem using graph of city nodes, with roads connecting as edges.\n",
    "    * Breadth-first search will consider all paths, which will result in an enormous number of paths, especially if considering all small roads. In addition, many routes lead the wrong way.\n",
    "    * Human logic: Look for major roadways going roughly the right direction. Ignore paths going too far away from the destination (or save them for later). Prioritize paths that appear that they are going to the destination. These are examples of **heuristics**.\n",
    "* **Heuristic**: Some additional piece of information - a rule, function or constraint - that informs an otherwise brute-force algorithm to act in a more optimal manner.\n",
    "\n",
    "### A* Search\n",
    "\n",
    "A* search is a common AI algorithm that could be applied to navigation.\n",
    "\n",
    "* Given a starting point and a destination point:\n",
    "    * Consider all possible branches to explore\n",
    "    * For each possible branch:\n",
    "        * Calculate a heuristic, like euclidean distance to the destination (Q: Can we use ML and a set of features to generate a heuristic?)\n",
    "    * (Q: Prune any branches based on one or more heuristics?)\n",
    "    * Prioritize search based on the above heuristic, repeating until destination is reached.\n",
    "* Because current best next step may not be on the optimal path, only use the heuristic as a means of prioritizing branches to explore, but do not actually prune out next steps yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Playing\n",
    "\n",
    "Navigation as a problem may be considered an optimazation problem. Whereas, game playing seems like it is more suited to the label AI since the problem requires us to react to or anticipate changes to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic Tac Toe\n",
    "\n",
    "Search space can be thought of as a tree of all possible board configurations. The rules of the game limit the number of possible board states. However, there are still many nodes to search over. The search space can be **pruned** using heuristics that take into consideration the game's rules:\n",
    "\n",
    "* Never make a move that would not result in a possible win\n",
    "* Knowing that you're playing against an opponent/adversary, don't make a move that would immediately result in the opponent winning the following move.\n",
    "\n",
    "The latter point above is the core thought behind **adversarial search**. We can use the **Mini-Max algorithm** to maximize the chances of winning on our turn and minimizing the chances of our opponent winning on their turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monty Hall Problem\n",
    "\n",
    "$$\n",
    "P(A) + P(B) + P(C) = 1 \\\\\n",
    "P(A) = \\frac{1}{3}, P(B) = \\frac{1}{3}, P(C) = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "The above is known as the **prior probability** - \"our best guess given no further evidence.\"\n",
    "\n",
    "In the scenario where we pick door A and door B is opened, we have an observation $\\mbox{Open}_B$. We can calculate the **posterior probability** of $P(\\mbox{Car}_B|\\mbox{Open}_B) = 0$ since there would be no way for the car to be behind door B if it was opened. It is considered posterior because it's the probability after an observation.\n",
    "\n",
    "We want to find the posterior probability that the car is behind door C given that Monty opened door B.\n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\mbox{Car}_C|\\mbox{Open}_B) &= \\frac{P(\\mbox{Open}_B|\\mbox{Car}_C) \\cdot P(\\mbox{Car}_C)}{P(\\mbox{Open}_B)} \\\\\n",
    "    &= \\frac{1 \\cdot \\frac{1}{3}}{P(\\mbox{Open}_B)} \\\\\n",
    "P(\\mbox{Open}_B) &= P(\\mbox{Car}_A) \\cdot P(\\mbox{Open}_B | \\mbox{Car}_A) \\\\\n",
    "    &+ P(\\mbox{Car}_B) \\cdot P(\\mbox{Open}_B | \\mbox{Car}_B) \\\\\n",
    "    &+ P(\\mbox{Car}_C) \\cdot P(\\mbox{Open}_B | \\mbox{Car}_C) \\\\\n",
    "    &= \\frac{1}{3} \\cdot \\frac{1}{2} + \\frac{1}{3} \\cdot 0 + \\frac{1}{3} \\cdot 1 \\\\\n",
    "    &= \\frac{1}{2} \\\\\n",
    "P(\\mbox{Car}_C|\\mbox{Open}_B) &= \\frac{1 \\cdot \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n",
    "\\end{align}$$\n",
    "\n",
    "$P(\\mbox{Car}_C|\\mbox{Open}_B) = \\frac{P(\\mbox{Open}_B|\\mbox{Car}_C) \\cdot P(\\mbox{Car}_C)}{P(\\mbox{Open}_B)}$ is an application of [Bayes' Theorem][]:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "$P(\\mbox{Open}_B)$ is known as the **marginal probability**. Marginal probability is the probability of an event occurring. It can even be thought of as the expected value or the unconditional probability.\n",
    "\n",
    "<!-- Links -->\n",
    "[Bayes' Theorem]: https://en.wikipedia.org/wiki/Bayes'_theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligence\n",
    "\n",
    "* Intelligence should be a property that emerges out of the system itself, not influenced by our perception.\n",
    "* Intelligence should be defined within the context of the task. Understanding the task is key to designing intelligent systems.\n",
    "* The field of Artificial General Intelligence attempts to create systems that are capable of intelligence at the human level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent, Environment, and State\n",
    "\n",
    "* Agent - the software itself (e.g. Roomba)\n",
    "* Environment - the task or problem domain with which the agent interacts with (e.g. Floors, Walls, Furniture, etc.)\n",
    "* State - only the information or data necessary for completing the task\n",
    "* Goal state - state or set of states that are considered the final state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perception, Action, and Cognition\n",
    "\n",
    "* Perception - when the agent uses sensors to interact with the environment, capturing data or state\n",
    "* Actions - outputs produced by the agent on the environment\n",
    "* Cognition - the decision making process the agent undergoes when determining what action to produce\n",
    "\n",
    "#### Types of Agents\n",
    "\n",
    "* Reactive agents - pre-programmed behavior to perceptions\n",
    "* Game-plaing agents\n",
    "* Path-finding agents\n",
    "* Knowledge-based agents\n",
    "* Planning agents\n",
    "* Learning-based agents\n",
    "* Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of AI Problems\n",
    "\n",
    "* Observability\n",
    "    * Fully observable (e.g. Tic Tac Toe)\n",
    "    * Partially observable (e.g. Battleship)\n",
    "* Determinism\n",
    "    * Deterministic - fully know the results of each action\n",
    "    * Stocahstic - Uncertainty in result of action. Randomness or chance is involved.\n",
    "* Continuity\n",
    "    * Discrete - finite number of states the environment could be in\n",
    "    * Continuous - infinite number of states, typically because some state is stored as real numbers\n",
    "* Benign vs adversarial\n",
    "    * Benign - The agent is the only one producing actions that _intentionally_ affect its goal\n",
    "    * Adversarial - Other agents are taking actions to intentionally against the agent's goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rational Behavior and Bounded Optimality\n",
    "\n",
    "* **Rational Behavior**: An intelligent agent is one that takes actions to optimize its expected utility, given a desired goal.\n",
    "* **Bounded Optimality**: Because we may not be able to always choose the very most optimal action, we can behave optimally within a practical bound (e.g. route finding algorithm should never be more than 2 miles of the optimal route)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
